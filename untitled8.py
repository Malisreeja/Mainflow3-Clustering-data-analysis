# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DUPKPO_lHt8QMgm0gZMpu8R30ItvMMUV
"""

# === Setup (Colab) ===
!pip -q install scikit-learn matplotlib seaborn pandas numpy

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE

from google.colab import files
import io, textwrap, sys, warnings
warnings.filterwarnings("ignore")

pd.set_option("display.max_columns", None)
sns.set(context="notebook", style="whitegrid")

# === Upload customer_data.csv from your computer ===
print("Upload your customer_data.csv file…")
uploaded = files.upload()

# If the file name differs, adjust below:
df = pd.read_csv(io.BytesIO(uploaded['customer_data.csv']))

print("Shape:", df.shape)
display(df.head())

print("\n--- Info ---")
df.info()

print("\n--- Missing values per column ---")
print(df.isna().sum())

print("\n--- Duplicate rows ---")
print(df.duplicated().sum())

print("\n--- Summary statistics ---")
display(df.describe(include='all').T)

# Drop full-row duplicates
before = df.shape[0]
df = df.drop_duplicates().reset_index(drop=True)
after = df.shape[0]
print(f"Removed {before - after} duplicate rows.")

# Simple missing handling strategy:
# - For numeric features: impute with median
# - For non-numeric: drop rows (rare here), or fill with mode (optional)
numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
non_numeric_cols = [c for c in df.columns if c not in numeric_cols]

for c in numeric_cols:
    if df[c].isna().sum() > 0:
        med = df[c].median()
        df[c] = df[c].fillna(med)

for c in non_numeric_cols:
    if df[c].isna().sum() > 0:
        mode_val = df[c].mode().iloc[0]
        df[c] = df[c].fillna(mode_val)

print("\nMissing after cleaning:")
print(df.isna().sum())

# Keep original copy for later joins/exports
df_orig = df.copy()

# Choose features for clustering:
feature_cols = ['Age', 'Annual Income', 'Spending Score']
for col in feature_cols:
    if col not in df.columns:
        raise ValueError(f"Missing expected column: {col}")

X = df[feature_cols].copy()

# Choose one scaler: StandardScaler or MinMaxScaler
use_minmax = False  # set True if you prefer 0-1 range

scaler = MinMaxScaler() if use_minmax else StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Scaled shape:", X_scaled.shape)

# Elbow: compute WCSS for k in 1..10 (or 12)
k_range = range(1, 11)
wcss = []
for k in k_range:
    km = KMeans(n_clusters=k, random_state=42, n_init='auto')
    km.fit(X_scaled)
    wcss.append(km.inertia_)

plt.figure(figsize=(6,4))
plt.plot(list(k_range), wcss, marker='o')
plt.title("Elbow Method (WCSS vs K)")
plt.xlabel("Number of clusters (k)")
plt.ylabel("WCSS (Inertia)")
plt.xticks(list(k_range))
plt.show()

# Silhouette: only valid for k >= 2
sil_scores = {}
for k in range(2, 11):
    km = KMeans(n_clusters=k, random_state=42, n_init='auto')
    labels = km.fit_predict(X_scaled)
    sil = silhouette_score(X_scaled, labels)
    sil_scores[k] = sil

print("\nSilhouette scores (higher is better):")
for k, s in sil_scores.items():
    print(f"k={k}: {s:.4f}")

# Pick the best k by silhouette (you can override manually if elbow suggests otherwise)
best_k_sil = max(sil_scores, key=sil_scores.get)
print(f"\nAuto-selected BEST_K by silhouette: {best_k_sil}")
BEST_K = best_k_sil

kmeans = KMeans(n_clusters=BEST_K, random_state=42, n_init='auto')
clusters = kmeans.fit_predict(X_scaled)

df_clustered = df_orig.copy()
df_clustered['Cluster'] = clusters

print("Cluster counts:")
display(df_clustered['Cluster'].value_counts().sort_index())

display(df_clustered.head())

out_csv = "customer_data_clustered.csv"
df_clustered.to_csv(out_csv, index=False)
print(f"Saved: {out_csv}")
files.download(out_csv)

# Reduce to 2D for visualization
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

# Centroids in PCA space
centroids_pca = pca.transform(kmeans.cluster_centers_)

plt.figure(figsize=(7,6))
scatter = plt.scatter(X_pca[:,0], X_pca[:,1], c=clusters, alpha=0.7)
plt.scatter(centroids_pca[:,0], centroids_pca[:,1], s=200, marker='X', edgecolor='black')
plt.title(f"PCA 2D — KMeans Clusters (k={BEST_K})")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.show()

# t-SNE is stochastic; for speed use perplexity ~ 30 for mid-sized data
tsne = TSNE(n_components=2, random_state=42, perplexity=30, learning_rate='auto', init='pca', n_iter=1000)
X_tsne = tsne.fit_transform(X_scaled)

plt.figure(figsize=(7,6))
scatter = plt.scatter(X_tsne[:,0], X_tsne[:,1], c=clusters, alpha=0.7)
plt.title(f"t-SNE 2D — KMeans Clusters (k={BEST_K})")
plt.xlabel("Dim 1")
plt.ylabel("Dim 2")
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.show()

# Pairplot (can be slow for large data)
tmp = df_clustered[feature_cols + ['Cluster']].copy()
tmp['Cluster'] = tmp['Cluster'].astype(str)  # seaborn likes str hue
sns.pairplot(tmp, hue='Cluster', corner=True, plot_kws=dict(alpha=0.7, s=25))
plt.suptitle("Pair Plot by Cluster", y=1.02)
plt.show()

# Cluster-wise feature means (original scale)
cluster_profile = df_clustered.groupby('Cluster')[feature_cols].mean().sort_index()
display(cluster_profile)

plt.figure(figsize=(6,4))
sns.heatmap(cluster_profile, annot=True, fmt=".1f", cmap="viridis")
plt.title("Cluster Profile — Mean Feature Values (Original Scale)")
plt.show()

# Inverse-transform centroids to original scales for easy reading
centroids_original = pd.DataFrame(
    scaler.inverse_transform(kmeans.cluster_centers_),
    columns=feature_cols
)
centroids_original['Cluster'] = range(BEST_K)
centroids_original = centroids_original.set_index('Cluster').sort_index()

print("Centroids (Original Scale):")
display(centroids_original.round(2))

# Rank clusters by income/spending to derive segment names
cent = centroids_original.copy()
cent['IncomeRank'] = cent['Annual Income'].rank(method='dense')
cent['SpendRank']  = cent['Spending Score'].rank(method='dense')

def name_segment(row):
    inc = "High Income" if row['IncomeRank'] >= cent['IncomeRank'].median() else "Low/Med Income"
    spend = "High Spending" if row['SpendRank'] >= cent['SpendRank'].median() else "Low/Med Spending"
    # refine with age
    age_tag = "Young" if row['Age'] < cent['Age'].median() else "Mature"
    return f"{inc} – {spend} ({age_tag})"

segment_names = cent.apply(name_segment, axis=1)
segment_map = segment_names.to_dict()

# Attach segment names to each customer
df_clustered['Segment'] = df_clustered['Cluster'].map(segment_map)

# High-level insights
summary = df_clustered.groupby(['Cluster','Segment']).agg(
    Customers=('Customer ID', 'count') if 'Customer ID' in df_clustered.columns else ('Segment','count'),
    Avg_Age=('Age','mean'),
    Avg_Income=('Annual Income','mean'),
    Avg_Spending=('Spending Score','mean')
).round(2)

print("\n=== Segment Summary ===")
display(summary)

# Recommendations based on segments
actions = []
for c, row in cent.iterrows():
    seg = segment_map[c]
    recs = []

    # Income-based
    if row['Annual Income'] >= cent['Annual Income'].median():
        recs.append("Upsell premium products; bundle high-value offers.")
    else:
        recs.append("Value-for-money bundles; discounts to increase basket size.")

    # Spending-based
    if row['Spending Score'] >= cent['Spending Score'].median():
        recs.append("Launch loyalty/early-access perks; VIP communications.")
    else:
        recs.append("Engage with targeted promos and onboarding nudges.")

    # Age nuance
    if row['Age'] < cent['Age'].median():
        recs.append("Use social-first, mobile offers; gamified rewards.")
    else:
        recs.append("Highlight reliability, support, and convenience.")

    actions.append({
        "Cluster": c,
        "Segment": seg,
        "Key Traits": f"Age≈{row['Age']:.0f}, Income≈{row['Annual Income']:.0f}, SpendScore≈{row['Spending Score']:.0f}",
        "Recommendations": " | ".join(recs)
    })

rec_df = pd.DataFrame(actions).sort_values("Cluster")
display(rec_df)

# Save outputs
df_clustered.to_csv("customer_data_clustered_with_segments.csv", index=False)
summary.to_csv("cluster_segment_summary.csv")
rec_df.to_csv("cluster_recommendations.csv", index=False)

print("\nSaved: customer_data_clustered_with_segments.csv, cluster_segment_summary.csv, cluster_recommendations.csv")
files.download("customer_data_clustered_with_segments.csv")
files.download("cluster_segment_summary.csv")
files.download("cluster_recommendations.csv")



"""#  Overall Summary

In this project, we carried out **Customer Segmentation using K-Means Clustering** on the dataset containing **Age, Annual Income, and Spending Score**.  

###  Data Preparation
- The dataset was inspected for **missing values, duplicates, and data types**.  
- After cleaning, the features were **standardized** to bring them to the same scale, ensuring clustering accuracy.  

###  Clustering Process
- The **Elbow Method (WCSS)** and **Silhouette Scores** were used to determine the optimal number of clusters.  
- **K-Means** was applied, and each customer was assigned a **Cluster label**.  

###  Visualizations
- **Elbow Plot:** Clearly showed the “elbow point” for selecting the best number of clusters.  
- **PCA Scatter Plot:** Reduced dimensions to 2D and displayed clusters in different colors along with centroids.  
- **t-SNE Plot:** Gave another perspective of how customers group together non-linearly.  
- **Pair Plot:** Showed relationships between Age, Income, and Spending Score within clusters.  
- **Heatmap of Centroids:** Displayed average feature values per cluster, making interpretation easier.  

###  Cluster Profiles
- Each cluster was summarized with **average age, income, and spending score**, helping to identify meaningful groups such as:  
  - **High Income – High Spending**  
  - **High Income – Low Spending**  
  - **Low Income – High Spending**  
  - **Young vs Mature segments**  

###  Business Insights & Recommendations
- Focus premium marketing strategies on **High Income – High Spending** customers.  
- Offer **loyalty programs** to retain high-spending groups.  
- Provide **discounts and value bundles** to attract low-income but high-spending clusters.  
- Engage **young customers** through mobile-first and social campaigns.  
- Build **trust and convenience services** for mature customers.  

---

###  Final Note
This clustering analysis produced **clean, well-structured data**, insightful **visualizations**, and **actionable strategies**.  
It enables businesses to:  
- **Understand customer behavior more clearly**  
- **Target the right groups effectively**  
- **Allocate resources wisely**  
- Ultimately drive **better marketing outcomes and customer satisfaction**.  

"""